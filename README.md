# Point-E Server

Production-ready GPU-accelerated server for OpenAI's Point-E text-to-3D generation model.

## Features

- üöÄ GPU-accelerated inference (30 seconds per generation)
- üì¶ Pre-cached models in Docker image for instant startup
- üîß RESTful API with automatic documentation
- ‚òÅÔ∏è Ready for Google Cloud deployment (GKE, Compute Engine)
- ‚ö° Auto-scaling support with load balancing
- üè† Local server with automatic device detection (GPU/MPS/CPU)
- üíæ Google Cloud Storage integration for production deployments

## Quick Start

### Local Development

Run Point-E locally on your machine with automatic device detection:

```bash
# Install dependencies
pip install -r requirements.txt
pip install git+https://github.com/openai/CLIP.git

# Run local server (auto-detects GPU/MPS/CPU)
python local_server.py
```

The local server will:
- ‚úÖ Automatically detect and use NVIDIA GPU if available
- ‚úÖ Use Apple Silicon GPU (MPS) on M1/M2 Macs
- ‚úÖ Fall back to CPU if no GPU is available
- üìÅ Save generated models to `generated_models/` directory

**Performance:**
- GPU (NVIDIA/Apple): ~30-60 seconds per model
- CPU: 2-10 minutes per model (varies by hardware)

### Docker (Production)

```bash
# Build image with pre-cached models
docker build -t point-e-server .

# Run with GPU
docker run --gpus all -p 8000:8000 point-e-server
```

## API Usage

### Generate 3D Model

```bash
curl -X POST http://localhost:8000/generate/text \
  -H "Content-Type: application/json" \
  -d '{"prompt": "a red chair", "user_id": "test-user"}'
```

### Check Device Info (Local Server)

```bash
curl http://localhost:8000/
```

Returns device being used (GPU/MPS/CPU) and output directory.

### Health Check

```bash
curl http://localhost:8000/health
```

### Browse Generated Models (Local Server)

Visit `http://localhost:8000/browse` to see all generated models.

### API Documentation

- üìñ **[Full API Reference](API.md)** - Complete documentation with examples
- üîß **Interactive Docs** - Visit `http://localhost:8000/docs` when server is running

## Deployment

### Google Cloud Platform Setup

1. **Configure your project**:
   ```bash
   cp config.sh.example config.sh
   # Edit config.sh with your GCP project ID and settings
   source config.sh
   ```

2. **Build and push the Docker image**:
   ```bash
   ./scripts/build-image.sh
   ```

3. **Deploy to GCP** (see deployment guide for full instructions):
   - Create GPU-enabled instance group
   - Set up load balancer
   - Configure auto-scaling

4. **Update deployment** (after code changes):
   ```bash
   ./scripts/update-point-e.sh
   ```

### Features

- **Google Cloud Storage**: Generated models are saved to GCS, organized by user
- **Auto-scaling**: Scales 1-5 instances based on load
- **GPU acceleration**: Uses NVIDIA L4 GPUs for ~30 second generation
- **Pre-cached models**: Fast startup times (~2 minutes)

## Which Server to Use?

- **`local_server.py`** - For local development and testing
  - Auto-detects GPU/MPS/CPU
  - Saves files locally
  - No cloud dependencies
  - Perfect for experimentation

- **`server.py`** - For production deployments
  - Optimized for Google Cloud
  - Supports Google Cloud Storage
  - Ready for auto-scaling
  - Load balancer compatible

## Project Structure

```
.
‚îú‚îÄ‚îÄ local_server.py        # Local development server
‚îú‚îÄ‚îÄ server.py              # Production server (GCS support)
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ Dockerfile            # Production Docker image
‚îú‚îÄ‚îÄ point_e/              # Point-E model source
‚îú‚îÄ‚îÄ k8s/                  # Kubernetes manifests
‚îú‚îÄ‚îÄ scripts/              # Deployment scripts
‚îú‚îÄ‚îÄ tests/                # API tests
‚îî‚îÄ‚îÄ generated_models/     # Local output directory
```

## Security Note

This project includes authentication and rate limiting features:
- **Shared Secret Authentication**: Set `POINT_E_SECRET_KEY` environment variable
- **Rate Limiting**: 5 generations per IP per day (configurable)
- **Safe for Open Source**: All sensitive configuration uses environment variables

Never commit your actual `config.sh` or secret keys to the repository!

## Server-to-Server Authentication

When calling the Point-E API from your main server, include these headers:

### Required Headers
```
Authorization: Bearer YOUR_SECRET_KEY
Content-Type: application/json
X-Forwarded-For: actual-user-ip  # Important for rate limiting
```

### Example Integration

```python
import requests

POINT_E_URL = "http://YOUR_LOAD_BALANCER_IP"
SECRET_KEY = "your-secret-key-from-config"

def generate_3d_from_main_server(prompt, user_ip, user_id):
    headers = {
        "Authorization": f"Bearer {SECRET_KEY}",
        "Content-Type": "application/json",
        "X-Forwarded-For": user_ip  # Pass the real user's IP
    }
    
    response = requests.post(
        f"{POINT_E_URL}/generate/text",
        headers=headers,
        json={"prompt": prompt, "user_id": user_id}
    )
    
    return response.json()
```

### Important Notes
- **X-Forwarded-For**: Must contain the actual end-user's IP address for proper rate limiting
- **Rate Limiting**: Default is 3 requests per IP per day (configurable)
- **User ID**: Include to organize generated files by user in GCS

## Configuration

### Setting up for your project

1. Copy `config.sh.example` to `config.sh`:
   ```bash
   cp config.sh.example config.sh
   ```

2. Edit `config.sh` with your Google Cloud project details:
   ```bash
   export GCP_PROJECT_ID="your-project-id"
   export POINT_E_SECRET_KEY="generate-a-secure-key"
   export RATE_LIMIT_PER_IP="3"
   ```

3. Source the configuration before running scripts:
   ```bash
   source config.sh
   ./scripts/build-image.sh
   ```

## License

MIT License - See LICENSE file for details